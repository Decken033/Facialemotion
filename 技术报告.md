#  技术报告

github开源库：https://github.com/MahmoudSabra1/Facial-emotion-recognition





数据集：

fer2013：[项目首页 - FER2013数据集原版CSV资源:本仓库提供了著名的**FER2013 (Facial Expression Recognition 2013)** 数据集的一个便捷下载链接。这个数据集是情感识别领域广泛使用的资源，特别适合于面部表情识别的研究与开发。数据集已经精心划分为了训练集和测试集，便于研究人员和开发者直接进行实验和应用。 - GitCode](https://gitcode.com/open-source-toolkit/ea5b2/?utm_source=tools_gitcode&index=top&type=card&webUrl&isLogin=1)

fer2013new：https://github.com/microsoft/FERPlus





错误：

1.模型在反序列化时找不到 `Sequential` 类。模型兼容性问题

![image-20250409141918326](C:\Users\asus\AppData\Roaming\Typora\typora-user-images\image-20250409141918326.png)

解决：使用的 Keras 或 TensorFlow 版本不兼容有关



![image-20250409233634194](C:\Users\asus\AppData\Roaming\Typora\typora-user-images\image-20250409233634194.png)



![image-20250409235338426](C:\Users\asus\AppData\Roaming\Typora\typora-user-images\image-20250409235338426.png)





![image-20250410000532931](C:\Users\asus\AppData\Roaming\Typora\typora-user-images\image-20250410000532931.png)



原始模型是一个卷积神经网络(CNN)，专门用于面部表情识别。从输入的48×48像素的人脸图像中，逐步提取特征并最终分类为7种不同的情绪。

```
输入: 48×48×1 灰度图像
│
├─► 第1阶段: 两层卷积+批归一化+Dropout
│    64个特征过滤器
│
├─► 第2阶段: 两层卷积+最大池化
│    64个特征过滤器
│    图像尺寸减半
│
├─► 第3阶段: 两层卷积+批归一化
│    128个特征过滤器
│
├─► 第4阶段: 两层卷积+最大池化
│    128个特征过滤器
│    图像尺寸再次减半
│
├─► 第5阶段: 两层卷积+批归一化
│    256个特征过滤器
│
├─► 扁平化: 将图像特征转换为一维向量
│
├─► 全连接层: 两层1024神经元+Dropout
│
└─► 输出层: 7个神经元(对应7种情绪)
     使用Softmax激活函数
```

## 为什么选择残差连接

**思考过程**：

1. **问题分析**：原始模型是一个较深的CNN（12层卷积+3层全连接），这种深度容易导致梯度消失问题。

2. **证据**：观察到原模型中已经使用了BatchNormalization，这表明设计者意识到了训练稳定性问题。

3. 方案对比

   ：

   - 减少网络深度：会降低模型容量，不可取
   - 使用更复杂的激活函数：可能有帮助但效果有限
   - 添加残差连接：直接解决梯度流动问题

4. **决策**：ResNet论文显示残差连接能让网络深度从几十层扩展到上百层，且实现简单、计算开销小，是最优选择。

## 为什么添加注意力机制

**思考过程**：

1. **任务特性分析**：面部表情识别需要关注面部的特定区域（眼睛、嘴角等）。

2. **原模型局限性**：标准CNN对整个图像区域平等对待，无法突出关键区域。

3. 可行方案

   ：

   - 人脸关键点检测：需要额外标注，复杂度高
   - 局部区域CNN：需要预定义区域，缺乏灵活性
   - 注意力机制：自动学习重要区域，无需额外标注

4. 机制选择

   ：

   - 空间注意力：关注"图像哪个位置"重要
   - 通道注意力：关注"哪些特征"重要
   - CBAM结合两者：最全面的方案，论文显示在多个视觉任务上有显著提升

## 为什么改进数据增强

**思考过程**：

1. **数据分析**：情绪识别数据集通常样本量有限，且类别分布不均衡。

2. **原始增强评估**：只使用了旋转、翻转和平移，增强多样性有限。

3. 增强策略考量

   ：

   - 需保持人脸基本结构：不能过度变形
   - 现实场景变化：光照条件变化（亮度）、距离变化（缩放）
   - 增强程度权衡：过强导致失真，过弱效果有限

4. 参数选择

   ：

   - 缩放范围0.1：保持适度变化而不破坏面部结构
   - 亮度范围[0.8, 1.2]：模拟常见光照变化范围

## 为什么处理类别不平衡

**思考过程**：

1. **问题识别**：情绪数据集中通常"快乐"和"中性"样本多，"厌恶"和"恐惧"样本少。

2. **不处理的后果**：模型会偏向预测多数类，导致少数类召回率低。

3. 解决方案对比

   ：

   - 欠采样多数类：丢失有用数据
   - 过采样少数类：可能导致过拟合
   - 生成合成样本(SMOTE)：复杂且可能引入噪声
   - 类别权重：简单高效，不改变数据本身

4. 实现方式

   ：

   - 使用sklearn的`compute_class_weight`自动计算反比于样本频率的权重
   - 应用于损失函数，增大对少数类错误预测的惩罚

## 为什么使用学习率调度和早停

**思考过程**：

1. 训练动态分析

   ：深度学习模型训练通常有三个阶段：

   - 初始快速学习阶段
   - 中期细微调整阶段
   - 后期过拟合风险阶段

2. 固定学习率问题

   ：

   - 大学习率：可能在最优点附近震荡
   - 小学习率：初期收敛太慢

3. 策略选择

   ：

   - 学习率衰减：固定衰减方案缺乏灵活性
   - ReduceLROnPlateau：基于性能指标动态调整，更加智能
   - patience=5：给予充分机会探索，但不至于等待过久

4. 早停合理性

   ：

   - 验证集性能是泛化能力的代理指标
   - patience=15：给予充分机会度过性能平台期
   - restore_best_weights=True：确保使用最佳模型，而非最后模型

## 为什么使用GlobalAveragePooling而非Flatten

**思考过程**：

1. 参数效率分析

   ：

   - 假设最后卷积层输出512个8×8特征图
   - Flatten：需要512×8×8×1024=33,554,432个参数
   - GlobalAveragePooling：仅需512×1024=524,288个参数

2. 功能比较

   ：

   - Flatten保留空间信息但容易过拟合
   - GlobalAveragePooling强制模型学习更有意义的特征表示

3. **证据支持**：NetworkInNetwork和GoogLeNet等现代架构都采用了这种方法

4. **实现决策**：减少64倍参数量且提高泛化能力，明显优于Flatten

## 整体架构设计的系统思考

最终的改进方案是基于系统性思考和渐进式改进原则：

1. **保留原架构的核心优势**：深度足够的CNN结构和BatchNormalization

2. 有针对性地解决主要瓶颈

   ：

   - 梯度流动问题→残差连接
   - 关键区域识别→注意力机制
   - 数据限制→增强的数据增强
   - 类别不平衡→类别权重
   - 训练过程优化→学习率调度+早停

3. 平衡计算效率与性能

   ：

   - GlobalAveragePooling减少参数
   - 每阶段2个残差块而非更多
   - 注意力机制放在每个阶段末尾而非每层后面

这种渐进式而系统化的改进方法，既保留了原始模型的优点，又针对性地解决了主要瓶颈，在不过度增加复杂度的情况下提升模型性能。
